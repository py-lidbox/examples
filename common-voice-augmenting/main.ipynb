{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Automatically reload imported modules that are changed outside this notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# More pixels in figures\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.dpi\"] = 200\n",
    "\n",
    "# Init PRNG with fixed seed for reproducibility\n",
    "import numpy as np\n",
    "np_rng = np.random.default_rng(1)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(np_rng.integers(0, tf.int64.max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio augmentation by random speed changes and random filtering\n",
    "\n",
    "**2020-11-10**\n",
    "\n",
    "\n",
    "This example expands `common-voice-small`, in which we talked about different ways of augmenting the dataset.\n",
    "Instead of simply copying samples, we can resample them randomly to make them a bit [faster or slower](https://www.isca-speech.org/archive/interspeech_2015/i15_3586.html).\n",
    "In addition, by applying random [finite impulse response](https://en.wikipedia.org/wiki/Finite_impulse_response) (FIR) filters on the signals, we can try to [simulate microphone differences](https://www.isca-speech.org/archive/Interspeech_2018/abstracts/1047.html).\n",
    "We'll apply these two augmentation techniques in this example and see if it is possible to improve on our previous results.\n",
    "\n",
    "`tf.data.Dataset` makes it easy to cache all raw audio samples into a single file, from which we can reload the whole dataset at each epoch.\n",
    "This means that we can reapply both random augmentation techniques at every epoch, hopefully with different output at each epoch.\n",
    "\n",
    "## Data\n",
    "\n",
    "This example uses the same data as in the `common-voice-small` example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "languages = \"\"\"\n",
    "    et\n",
    "    mn\n",
    "    ta\n",
    "    tr\n",
    "\"\"\".split()\n",
    "\n",
    "languages = sorted(l.strip() for l in languages)\n",
    "\n",
    "display(Markdown(\"### Languages\"))\n",
    "display(Markdown('\\n'.join(\"* `{}`\".format(l) for l in languages)))\n",
    "\n",
    "bcp47_validator_url = 'https://schneegans.de/lv/?tags='\n",
    "display(Markdown(\"See [this tool]({}) for a description of the BCP-47 language codes.\"\n",
    "                 .format(bcp47_validator_url + urllib.parse.quote('\\n'.join(languages)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "workdir = \"/data/exp/cv4-augment\"\n",
    "datadir = \"/mnt/data/speech/common-voice/downloads/2020/cv-corpus\"\n",
    "\n",
    "print(\"work dir:\", workdir)\n",
    "print(\"data source dir:\", datadir)\n",
    "print()\n",
    "\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "assert os.path.isdir(datadir), datadir + \" does not exist\"\n",
    "\n",
    "dirs = sorted((f for f in os.scandir(datadir) if f.is_dir()), key=lambda f: f.name)\n",
    "\n",
    "print(datadir)\n",
    "for d in dirs:\n",
    "    if d.name in languages:\n",
    "        print(' ', d.name)\n",
    "        for f in os.scandir(d):\n",
    "            print('   ', f.name)\n",
    "\n",
    "missing_languages = set(languages) - set(d.name for d in dirs)\n",
    "assert missing_languages == set(), \"missing languages: {}\".format(missing_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lidbox.meta import common_voice, generate_label2target\n",
    "\n",
    "\n",
    "meta = common_voice.load_all(datadir, languages)\n",
    "meta, lang2target = generate_label2target(meta)\n",
    "\n",
    "print(\"lang2target\")\n",
    "for l, t in lang2target.items():\n",
    "    print(\"  {}: {}\".format(l, t))\n",
    "\n",
    "for split in meta.split.unique():\n",
    "    display(Markdown(\"### \" + split))\n",
    "    display(meta[meta[\"split\"]==split])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the metadata is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lidbox.meta import verify_integrity\n",
    "\n",
    "\n",
    "print(\"size of all metadata\", meta.shape)\n",
    "meta = meta.dropna()\n",
    "print(\"after dropping NaN rows\", meta.shape)\n",
    "\n",
    "print(\"verifying integrity\")\n",
    "verify_integrity(meta)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing the language distribution\n",
    "\n",
    "We'll repeat the same random oversampling by audio sample length procedure as we did in `common-voice-small`.\n",
    "This time, we add a flag `is_copy == True` to each oversampled copy, which allows us to easily filter all copies when we do random speed changes on the audio signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from lidbox.meta import read_audio_durations, random_oversampling\n",
    "from lidbox.visualize import plot_duration_distribution\n",
    "\n",
    "\n",
    "meta[\"duration\"] = read_audio_durations(meta)\n",
    "\n",
    "# Flag for distinguishing original rows from copies produced by oversampling\n",
    "# This is also used later for random resampling of signals\n",
    "meta = meta.assign(is_copy=False)\n",
    "train, rest = meta[meta[\"split\"]==\"train\"], meta[meta[\"split\"]!=\"train\"]\n",
    "augmented_train = random_oversampling(train, copy_flag=\"is_copy\", random_state=np_rng.bit_generator)\n",
    "meta = pd.concat([augmented_train, rest], verify_integrity=True).sort_index()\n",
    "verify_integrity(meta)\n",
    "\n",
    "sns.set(rc={})\n",
    "plot_duration_distribution(meta)\n",
    "for split in meta.split.unique():\n",
    "    display(Markdown(\"### \" + split))\n",
    "    display(meta[meta[\"split\"]==split])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "samples = (meta[meta[\"split\"]==\"train\"]\n",
    "           .groupby(\"label\")\n",
    "           .sample(n=2, random_state=np_rng.bit_generator))\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lidbox.features import audio\n",
    "from lidbox.visualize import plot_signal\n",
    "from IPython.display import display, Audio, HTML\n",
    "\n",
    "\n",
    "def read_mp3(path):\n",
    "    s, rate = audio.read_mp3(path)\n",
    "    out_rate = 16000\n",
    "    s = audio.resample(s, rate, out_rate)\n",
    "    s = audio.peak_normalize(s, dBFS=-3.0)\n",
    "    s = audio.remove_silence(s, out_rate)\n",
    "    return s, out_rate\n",
    "\n",
    "def embed_audio(signal, rate):\n",
    "    display(Audio(data=signal, rate=rate, embed=True, normalize=False))\n",
    "\n",
    "def plot_separator():\n",
    "    display(HTML(data=\"<hr style='border: 2px solid'>\"))\n",
    "\n",
    "    \n",
    "for sentence, lang, clip_path in samples[[\"sentence\", \"label\", \"path\"]].to_numpy():\n",
    "    signal, rate = read_mp3(clip_path)\n",
    "    signal = signal.numpy()\n",
    "    plot_signal(signal)\n",
    "    print(\"length: {} sec\".format(signal.size / rate))\n",
    "    print(\"lang:\", lang)\n",
    "    print(\"sentence:\", sentence)\n",
    "    embed_audio(signal, rate)\n",
    "    plot_separator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random filtering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "\n",
    "\n",
    "def random_filter(s, N=10):\n",
    "    b = np_rng.normal(0, 1, N)\n",
    "    return scipy.signal.lfilter(b, 1.0, s).astype(np.float32), b\n",
    "\n",
    "def display_signal(s, r, l):\n",
    "    plot_signal(s)\n",
    "    print(\"length: {} sec\".format(s.size / r))\n",
    "    print(\"lang:\", l)\n",
    "    embed_audio(s, r)\n",
    "    plot_separator()\n",
    "\n",
    "    \n",
    "sentence, lang, path = samples[[\"sentence\", \"label\", \"path\"]].to_numpy()[2]\n",
    "signal, rate = read_mp3(path)\n",
    "signal = audio.remove_silence(signal, rate).numpy()\n",
    "\n",
    "print(\"original\")\n",
    "display_signal(signal, rate, lang)\n",
    "\n",
    "np.set_printoptions(precision=1)\n",
    "\n",
    "for _ in range(5):\n",
    "    s, b = random_filter(signal)\n",
    "    print(\"filter:\", b)\n",
    "    s = audio.peak_normalize(s, dBFS=-3.0).numpy()\n",
    "    display_signal(s, rate, lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random speed change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def random_speed_change(s, r, lo=0.9, hi=1.1):\n",
    "    ratio = np_rng.uniform(lo, hi)\n",
    "    new_len = int(len(s) * r / (ratio * r))\n",
    "    return scipy.signal.resample(s, new_len).astype(np.float32), ratio\n",
    "    \n",
    "\n",
    "print(\"original\")\n",
    "display_signal(signal, rate, lang)\n",
    "\n",
    "for ratio in [0.9, 0.95, 1, 1.05, 1.1]:\n",
    "    s, ratio = random_speed_change(signal, rate, lo=ratio, hi=ratio)\n",
    "    print(\"speed ratio: {:.3f}\".format(ratio))\n",
    "    display_signal(s, rate, lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lidbox.features import audio, cmvn\n",
    "\n",
    "\n",
    "TF_AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "def metadata_to_dataset_input(meta):   \n",
    "    return {\n",
    "        \"id\": tf.constant(meta.index, tf.string),\n",
    "        \"path\": tf.constant(meta.path, tf.string),\n",
    "        \"label\": tf.constant(meta.label, tf.string),\n",
    "        \"target\": tf.constant(meta.target, tf.int32),\n",
    "        \"split\": tf.constant(meta.split, tf.string),\n",
    "        \"is_copy\": tf.constant(meta.is_copy, tf.bool),\n",
    "    }\n",
    "\n",
    "\n",
    "def read_mp3(x):\n",
    "    s, r = audio.read_mp3(x[\"path\"])\n",
    "    out_rate = 16000\n",
    "    s = audio.resample(s, r, out_rate)\n",
    "    s = audio.peak_normalize(s, dBFS=-3.0)\n",
    "    s = audio.remove_silence(s, out_rate)\n",
    "    return dict(x, signal=s, sample_rate=out_rate)\n",
    "\n",
    "\n",
    "def random_speed_change_wrapper(x):\n",
    "    if not x[\"is_copy\"]:\n",
    "        return x\n",
    "    s, _ = tf.numpy_function(\n",
    "        random_speed_change,\n",
    "        [x[\"signal\"], x[\"sample_rate\"]],\n",
    "        [tf.float32, tf.float64],\n",
    "        name=\"np_random_speed_change\")\n",
    "    return dict(x, signal=s)\n",
    "\n",
    "\n",
    "def random_filter_wrapper(x):\n",
    "    s, _ = tf.numpy_function(\n",
    "        random_filter,\n",
    "        [x[\"signal\"]],\n",
    "        [tf.float32, tf.float64],\n",
    "        name=\"np_random_filter\")\n",
    "    s = tf.cast(s, tf.float32)\n",
    "    s = audio.peak_normalize(s, dBFS=-3.0)\n",
    "    return dict(x, signal=s)\n",
    "\n",
    "\n",
    "def batch_extract_features(x):\n",
    "    with tf.device(\"GPU\"):\n",
    "        signals, rates = x[\"signal\"], x[\"sample_rate\"]\n",
    "        S = audio.spectrograms(signals, rates[0])\n",
    "        S = audio.linear_to_mel(S, rates[0])\n",
    "        S = tf.math.log(S + 1e-6)\n",
    "        S = cmvn(S, normalize_variance=False)\n",
    "    return dict(x, logmelspec=S)\n",
    "\n",
    "\n",
    "def signal_is_not_empty(x):\n",
    "    return tf.size(x[\"signal\"]) > 0\n",
    "\n",
    "\n",
    "def pipeline_from_metadata(data, split):\n",
    "    if split == \"train\":\n",
    "        data = data.sample(frac=1)\n",
    "    ds = (\n",
    "        tf.data.Dataset.from_tensor_slices(metadata_to_dataset_input(data))\n",
    "        .map(read_mp3, num_parallel_calls=TF_AUTOTUNE)\n",
    "        .filter(signal_is_not_empty)\n",
    "        # Try to keep 1000 signals prefetched in an in-memory buffer to reduce downstream latency\n",
    "        .prefetch(1000)\n",
    "        # Cache signals to a single file\n",
    "        .cache(os.path.join(cachedir, \"data\", split))\n",
    "        # In-memory buffer when reading from the cache\n",
    "        .prefetch(1000))\n",
    "    if split == \"train\":\n",
    "        ds = (ds\n",
    "              # Randomly change speed of all oversampled copies\n",
    "              .map(random_speed_change_wrapper, num_parallel_calls=TF_AUTOTUNE)\n",
    "              # Apply random filter for every training sample\n",
    "              .map(random_filter_wrapper, num_parallel_calls=TF_AUTOTUNE))\n",
    "    return (ds\n",
    "        .batch(1)\n",
    "        .map(batch_extract_features, num_parallel_calls=TF_AUTOTUNE)\n",
    "        .unbatch())\n",
    "\n",
    "\n",
    "cachedir = os.path.join(workdir, \"cache\")\n",
    "os.makedirs(os.path.join(cachedir, \"data\"))\n",
    "\n",
    "split2ds = {\n",
    "    split: pipeline_from_metadata(meta[meta[\"split\"]==split], split)\n",
    "    for split in meta.split.unique()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exhaust iterators to collect all audio into binary files\n",
    "\n",
    "**NOTE** that this creates 7.2 GiB of additional data on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import lidbox.data.steps as ds_steps\n",
    "\n",
    "\n",
    "for split, ds in split2ds.items():\n",
    "    print(\"filling\", split, \"cache\")\n",
    "    _ = ds_steps.consume(ds, log_interval=2000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect dataset contents in TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, ds in split2ds.items():\n",
    "    _ = ds_steps.consume_to_tensorboard(\n",
    "            ds.map(lambda x: dict(x, input=x[\"logmelspec\"])),\n",
    "            os.path.join(cachedir, \"tensorboard\", \"data\", split),\n",
    "            {\"batch_size\": 1,\n",
    "             \"image_size_multiplier\": 2,\n",
    "             \"num_batches\": 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a supervised, neural network language classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lidbox.models.xvector as xvector\n",
    "\n",
    "\n",
    "def create_model(num_freq_bins, num_labels):\n",
    "    model = xvector.create([None, num_freq_bins], num_labels, channel_dropout_rate=0.8)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model(\n",
    "    num_freq_bins=40,\n",
    "    num_labels=len(lang2target))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def as_model_input(x):\n",
    "    return x[\"logmelspec\"], x[\"target\"]\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    # Write scalar metrics and network weights to TensorBoard\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(cachedir, \"tensorboard\", model.name),\n",
    "        update_freq=\"epoch\",\n",
    "        write_images=True,\n",
    "        profile_batch=0,\n",
    "    ),\n",
    "    # Stop training if validation loss has not improved from the global minimum in 10 epochs\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "    ),\n",
    "    # Write model weights to cache everytime we get a new global minimum loss value\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(cachedir, \"model\", model.name),\n",
    "        monitor='val_loss',\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "train_ds = split2ds[\"train\"].map(as_model_input).shuffle(1000)\n",
    "dev_ds = split2ds[\"dev\"].map(as_model_input)\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds.batch(1),\n",
    "    validation_data=dev_ds.batch(1),\n",
    "    callbacks=callbacks,\n",
    "    verbose=2,\n",
    "    epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lidbox.util import evaluate_testset_with_model\n",
    "from lidbox.visualize import draw_confusion_matrix\n",
    "\n",
    "\n",
    "_ = model.load_weights(os.path.join(cachedir, \"model\", model.name))\n",
    "\n",
    "report = evaluate_testset_with_model(\n",
    "    model=model,\n",
    "    test_ds=split2ds[\"test\"].map(lambda x: dict(x, input=x[\"logmelspec\"])).batch(1),\n",
    "    test_meta=meta[meta[\"split\"]==\"test\"],\n",
    "    lang2target=lang2target)\n",
    "\n",
    "for m in (\"avg_detection_cost\", \"avg_equal_error_rate\", \"accuracy\"):\n",
    "    print(\"{}: {:.3f}\".format(m, report[m]))\n",
    "    \n",
    "lang_metrics = pd.DataFrame.from_dict({k: v for k, v in report.items() if k in lang2target})\n",
    "lang_metrics[\"mean\"] = lang_metrics.mean(axis=1)\n",
    "display(lang_metrics.T)\n",
    "\n",
    "fig, ax = draw_confusion_matrix(report[\"confusion_matrix\"], lang2target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Comparing to our previous example with the same dataset of 4 different languages (`common-voice-small`), the $\\text{C}_\\text{avg}$ value improved from 0.112 to 0.091 and accuracy from 0.803 to 0.846.\n",
    "\n",
    "Even though it is tempting to conclude that our augmentation approach was the cause of this improvement, we should probably perform hundreds of experiments with carefully chosen configuration settings to get a reliable answer if augmentation is useful or not."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
