{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Automatically reload imported modules that are changed outside this notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# More pixels in figures\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.dpi\"] = 200\n",
    "\n",
    "# Init PRNG with fixed seed for reproducibility\n",
    "import numpy as np\n",
    "np_rng = np.random.default_rng(1)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(np_rng.integers(0, tf.int64.max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language vectors, recurrent neural networks, and an angular proximity loss function\n",
    "\n",
    "**2020-11-21**\n",
    "\n",
    "In this example, we take a different approach for training language vectors (embeddings) compared to `common-voice-embeddings`.\n",
    "Previously, we trained a neural network on a classification task and used one of its layers as the representation for different classes.\n",
    "In this example, we train a neural network directly on the language vector task by maximizing the angular distance between vectors of different classes.\n",
    "We'll be using the approach described by [G. Gelly and J.L. Gauvain](https://www.isca-speech.org/archive/Interspeech_2017/abstracts/1334.html).\n",
    "\n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "We will continue with the same, 4-language Common Voice data as in all previous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "languages = \"\"\"\n",
    "    et\n",
    "    mn\n",
    "    ta\n",
    "    tr\n",
    "\"\"\".split()\n",
    "\n",
    "languages = sorted(l.strip() for l in languages)\n",
    "\n",
    "display(Markdown(\"### Languages\"))\n",
    "display(Markdown('\\n'.join(\"* `{}`\".format(l) for l in languages)))\n",
    "\n",
    "bcp47_validator_url = 'https://schneegans.de/lv/?tags='\n",
    "display(Markdown(\"See [this tool]({}) for a description of the BCP-47 language codes.\"\n",
    "                 .format(bcp47_validator_url + urllib.parse.quote('\\n'.join(languages)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing the metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from lidbox.meta import (\n",
    "    common_voice,\n",
    "    generate_label2target,\n",
    "    verify_integrity,\n",
    "    read_audio_durations,\n",
    "    random_oversampling_on_split\n",
    ")\n",
    "\n",
    "\n",
    "workdir = \"/data/exp/cv4-angular-lstm\"\n",
    "datadir = \"/mnt/data/speech/common-voice/downloads/2020/cv-corpus\"\n",
    "\n",
    "print(\"work dir:\", workdir)\n",
    "print(\"data source dir:\", datadir)\n",
    "print()\n",
    "\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "assert os.path.isdir(datadir), datadir + \" does not exist\"\n",
    "\n",
    "dirs = sorted((f for f in os.scandir(datadir) if f.is_dir()), key=lambda f: f.name)\n",
    "\n",
    "print(datadir)\n",
    "for d in dirs:\n",
    "    if d.name in languages:\n",
    "        print(' ', d.name)\n",
    "        for f in os.scandir(d):\n",
    "            print('   ', f.name)\n",
    "\n",
    "missing_languages = set(languages) - set(d.name for d in dirs)\n",
    "assert missing_languages == set(), \"missing languages: {}\".format(missing_languages)\n",
    "\n",
    "meta = common_voice.load_all(datadir, languages)\n",
    "meta, lang2target = generate_label2target(meta)\n",
    "\n",
    "print(\"\\nsize of all metadata\", meta.shape)\n",
    "meta = meta.dropna()\n",
    "print(\"after dropping NaN rows\", meta.shape)\n",
    "\n",
    "print(\"verifying integrity\")\n",
    "verify_integrity(meta)\n",
    "print(\"ok\\n\")\n",
    "\n",
    "print(\"reading audio durations\")\n",
    "meta[\"duration\"] = read_audio_durations(meta)\n",
    "print(\"balancing the label distributions\")\n",
    "meta = random_oversampling_on_split(meta, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the feature extraction pipeline\n",
    "\n",
    "Most of the preprocessing will be as in `common-voice-embeddings`, but this time we will not be training on samples with varying length.\n",
    "\n",
    "We will make these changes:\n",
    "* Signals will be divided into 3.2 second chunks, with 75% overlap, as suggested in the [paper](https://www.isca-speech.org/archive/Interspeech_2017/abstracts/1334.html).\n",
    "* Every signal that is shorter than 3.2 seconds will be repeatedly appended to itself until it is at least 3.2 seconds long.\n",
    "* Random speed changes are applied only once, before caching the training set signals to disk. This is because `tf.keras.Model.fit` assumes the training set length does not change. This could probably be fixed by writing a custom training loop but we won't be doing that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "\n",
    "from lidbox.features import audio, cmvn\n",
    "import lidbox.data.steps as ds_steps\n",
    "\n",
    "\n",
    "TF_AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "def metadata_to_dataset_input(meta):   \n",
    "    return {\n",
    "        \"id\": tf.constant(meta.index, tf.string),\n",
    "        \"path\": tf.constant(meta.path, tf.string),\n",
    "        \"label\": tf.constant(meta.label, tf.string),\n",
    "        \"target\": tf.constant(meta.target, tf.int32),\n",
    "        \"split\": tf.constant(meta.split, tf.string),\n",
    "        \"is_copy\": tf.constant(meta.is_copy, tf.bool),\n",
    "    }\n",
    "\n",
    "\n",
    "def read_mp3(x):\n",
    "    s, r = audio.read_mp3(x[\"path\"])\n",
    "    out_rate = 16000\n",
    "    s = audio.resample(s, r, out_rate)\n",
    "    s = audio.peak_normalize(s, dBFS=-3.0)\n",
    "    s = audio.remove_silence(s, out_rate)\n",
    "    return dict(x, signal=s, sample_rate=out_rate)\n",
    "\n",
    "\n",
    "def random_filter(x):\n",
    "    def scipy_filter(s, N=10):\n",
    "        b = np_rng.normal(0, 1, N)\n",
    "        return scipy.signal.lfilter(b, 1.0, s).astype(np.float32), b\n",
    "    s, _ = tf.numpy_function(\n",
    "        scipy_filter,\n",
    "        [x[\"signal\"]],\n",
    "        [tf.float32, tf.float64],\n",
    "        name=\"np_random_filter\")\n",
    "    s = tf.cast(s, tf.float32)\n",
    "    s = audio.peak_normalize(s, dBFS=-3.0)\n",
    "    return dict(x, signal=s)\n",
    "\n",
    "\n",
    "def random_speed_change(ds):\n",
    "    return ds_steps.random_signal_speed_change(ds, min=0.9, max=1.1, flag=\"is_copy\")\n",
    "\n",
    "\n",
    "def create_signal_chunks(ds):\n",
    "    ds = ds_steps.repeat_too_short_signals(ds, 3200)\n",
    "    ds = ds_steps.create_signal_chunks(ds, 3200, 800)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def batch_extract_features(x):\n",
    "    with tf.device(\"GPU\"):\n",
    "        signals, rates = x[\"signal\"], x[\"sample_rate\"]\n",
    "        S = audio.spectrograms(signals, rates[0])\n",
    "        S = audio.linear_to_mel(S, rates[0])\n",
    "        S = tf.math.log(S + 1e-6)\n",
    "        S = cmvn(S, normalize_variance=False)\n",
    "    return dict(x, logmelspec=S)\n",
    "\n",
    "\n",
    "def pipeline_from_meta(data, split):\n",
    "    if split == \"train\":\n",
    "        data = data.sample(frac=1, random_state=np_rng.bit_generator)\n",
    "\n",
    "    ds = (tf.data.Dataset\n",
    "            .from_tensor_slices(metadata_to_dataset_input(data))\n",
    "            .map(read_mp3, num_parallel_calls=TF_AUTOTUNE))\n",
    "\n",
    "    if split == \"train\":\n",
    "        return (ds\n",
    "            .apply(random_speed_change)\n",
    "            .cache(os.path.join(cachedir, \"data\", split))\n",
    "            .prefetch(100)\n",
    "            .map(random_filter, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .apply(create_signal_chunks)\n",
    "            .batch(100)\n",
    "            .map(batch_extract_features, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .unbatch())\n",
    "    else:\n",
    "        return (ds\n",
    "            .apply(create_signal_chunks)\n",
    "            .batch(100)\n",
    "            .map(batch_extract_features, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .unbatch()\n",
    "            .cache(os.path.join(cachedir, \"data\", split))\n",
    "            .prefetch(100))\n",
    "\n",
    "\n",
    "cachedir = os.path.join(workdir, \"cache\")\n",
    "os.makedirs(os.path.join(cachedir, \"data\"))\n",
    "\n",
    "split2ds = {split: pipeline_from_meta(meta[meta[\"split\"]==split], split)\n",
    "            for split in meta.split.unique()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling the caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, ds in split2ds.items():\n",
    "    print(\"filling\", split, \"cache\")\n",
    "    _ = ds_steps.consume(ds, log_interval=5000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the LSTM model with angular proximity loss\n",
    "\n",
    "`lidbox` implements both the model and the angular proximity loss function used in the reference paper.\n",
    "The loss function aims to maximize the cosine distance of language vectors of different languages and minimize the distance for vectors of the same language.\n",
    "Reference vectors will be generated for each class such that all reference vectors are orthogonal to each other.\n",
    "\n",
    "\n",
    "In addition, we'll add [random channel dropout](https://dl.acm.org/doi/abs/10.1016/j.patrec.2017.09.023) to avoid overfitting on noise, as in the `common-voice-small` example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lidbox.models import ap_lstm\n",
    "from lidbox.losses import SparseAngularProximity\n",
    "\n",
    "\n",
    "def create_model(num_freq_bins=40, num_labels=len(lang2target)):\n",
    "    m = ap_lstm.create(\n",
    "        input_shape=[None, num_freq_bins],\n",
    "        num_outputs=num_labels,\n",
    "        num_lstm_units=200,\n",
    "        channel_dropout_rate=0.8)\n",
    "    m.compile(\n",
    "        loss=SparseAngularProximity(num_labels, m.output.shape[1]),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3))\n",
    "    return m\n",
    "\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(cachedir, \"tensorboard\", model.name),\n",
    "        update_freq=\"epoch\",\n",
    "        write_images=True,\n",
    "        profile_batch=0,\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(cachedir, \"model\", model.name),\n",
    "        monitor='val_loss',\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "def as_model_input(x):\n",
    "    return x[\"logmelspec\"], x[\"target\"]\n",
    "\n",
    "\n",
    "train_ds = split2ds[\"train\"].map(as_model_input).shuffle(5000)\n",
    "dev_ds = split2ds[\"dev\"].map(as_model_input)\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds.batch(32),\n",
    "    validation_data=dev_ds.batch(32),\n",
    "    callbacks=callbacks,\n",
    "    verbose=2,\n",
    "    epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating as an end-to-end classifier\n",
    "\n",
    "The angular proximity loss function uses reference directions for each language, such that each direction is orthogonal to each other.\n",
    "By selecting the closest reference direction for every predicted language vector, the model can be used as an end-to-end classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from lidbox.util import predict_with_model, classification_report\n",
    "from lidbox.visualize import draw_confusion_matrix\n",
    "\n",
    "\n",
    "def load_trained_model():\n",
    "    model = create_model()\n",
    "    model.load_weights(os.path.join(cachedir, \"model\", model.name))\n",
    "    return model\n",
    "\n",
    "\n",
    "def display_classification_report(report):\n",
    "    for m in (\"avg_detection_cost\", \"avg_equal_error_rate\", \"accuracy\"):\n",
    "        print(\"{}: {:.3f}\".format(m, report[m]))\n",
    "\n",
    "    lang_metrics = pd.DataFrame.from_dict(\n",
    "        {k: v for k, v in report.items() if k in lang2target})\n",
    "    lang_metrics[\"mean\"] = lang_metrics.mean(axis=1)\n",
    "    display(lang_metrics.T)\n",
    "\n",
    "    fig, ax = draw_confusion_matrix(report[\"confusion_matrix\"], lang2target)\n",
    "\n",
    "\n",
    "model = load_trained_model()\n",
    "\n",
    "\n",
    "def predict_with_ap_loss(x):\n",
    "    with tf.device(\"GPU\"):\n",
    "        # Generate language vector for input spectra\n",
    "        language_vector = model(x[\"input\"], training=False)\n",
    "        # Predict languages by computing distances to reference directions\n",
    "        return x[\"id\"], model.loss.predict(language_vector)\n",
    "\n",
    "\n",
    "chunk2pred = predict_with_model(\n",
    "    model=model,\n",
    "    ds=split2ds[\"test\"].map(lambda x: dict(x, input=x[\"logmelspec\"])).batch(128),\n",
    "    predict_fn=predict_with_ap_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging chunk predictions\n",
    "\n",
    "We divided all samples into 3.2 second chunks, so all predictions are still for these chunks.\n",
    "Lets merge all chunk predictions by taking the average over all chunks for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk2pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lidbox.util import merge_chunk_predictions\n",
    "\n",
    "\n",
    "utt2pred = merge_chunk_predictions(chunk2pred)\n",
    "utt2pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_meta = meta[meta[\"split\"]==\"test\"].join(utt2pred, how=\"outer\")\n",
    "assert not test_meta.isna().any(axis=None), \"failed to join predictions\"\n",
    "\n",
    "true_sparse = test_meta.target.to_numpy(np.int32)\n",
    "pred_dense = np.stack(test_meta.prediction)\n",
    "\n",
    "report = classification_report(true_sparse, pred_dense, lang2target)\n",
    "display_classification_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting all data as language vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lidbox.util import model2function\n",
    "\n",
    "\n",
    "extractor = model2function(load_trained_model())\n",
    "print(\"extractor:\", str(extractor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lidbox.visualize import plot_embedding_vector\n",
    "\n",
    "\n",
    "def is_not_copy(x):\n",
    "    return not x[\"is_copy\"]\n",
    "\n",
    "def batch_extract_embeddings(x):\n",
    "    with tf.device(\"GPU\"):\n",
    "        return dict(x, embedding=extractor(x[\"logmelspec\"]))\n",
    "\n",
    "\n",
    "embedding_demo_ds = (split2ds[\"train\"]\n",
    "                     .filter(is_not_copy)\n",
    "                     .take(12)\n",
    "                     .batch(1)\n",
    "                     .map(batch_extract_embeddings)\n",
    "                     .unbatch())\n",
    "\n",
    "for x in embedding_demo_ds.as_numpy_iterator():\n",
    "    print(x[\"id\"].decode(\"utf-8\"), x[\"embedding\"].shape)\n",
    "    plot_embedding_vector(x[\"embedding\"], figsize=(10, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a language vector extractor pipeline\n",
    "\n",
    "We'll now extend the existing feature extraction pipeline by adding a step where we extract language vectors with the trained model.\n",
    "In addition, we merge all chunks of each sample by summing over all components of its chunk vectors.\n",
    "The vector is then L2-normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from lidbox.util import predictions_to_dataframe\n",
    "\n",
    "\n",
    "# Merge chunk vectors by taking the sum over each component and L2-normalizing the result\n",
    "def sum_and_normalize(pred):\n",
    "    v = np.stack(pred).sum(axis=0)\n",
    "    v = normalize(v.reshape((1, -1)), axis=1)\n",
    "    return np.squeeze(v)\n",
    "\n",
    "\n",
    "def ds_to_embeddings(ds):\n",
    "    to_pair = lambda x: (x[\"id\"], x[\"embedding\"])\n",
    "    ds = (ds\n",
    "        .batch(128)\n",
    "        .map(batch_extract_embeddings, num_parallel_calls=TF_AUTOTUNE)\n",
    "        .unbatch()\n",
    "        .map(to_pair, num_parallel_calls=TF_AUTOTUNE))\n",
    "\n",
    "    ids = []\n",
    "    embeddings = []\n",
    "    \n",
    "    for id, embedding in ds.as_numpy_iterator():\n",
    "        ids.append(id.decode(\"utf-8\"))\n",
    "        embeddings.append(embedding.astype(np.float32))\n",
    "        \n",
    "    df = predictions_to_dataframe(ids, embeddings)\n",
    "    return merge_chunk_predictions(df, merge_rows_fn=sum_and_normalize)\n",
    "\n",
    "\n",
    "embeddings_by_split = (ds_to_embeddings(ds) for ds in split2ds.values())\n",
    "m = meta.join(pd.concat(embeddings_by_split, verify_integrity=True), how=\"outer\")\n",
    "assert not m.prediction.isna().any(axis=None), \"Missing embeddings, some rows contained NaN values\"\n",
    "\n",
    "meta = m.rename(columns={\"prediction\": \"embedding\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the language vectors for back-end training\n",
    "\n",
    "Now, let's extract all embeddings and integer targets into NumPy-data and preprocess them with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from lidbox.embed.sklearn_utils import PLDA\n",
    "\n",
    "\n",
    "def embeddings_as_numpy_data(df):\n",
    "    X = np.stack(df.embedding.values).astype(np.float32)\n",
    "    y = df.target.to_numpy(dtype=np.int32)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def random_sample(X, y, sample_size_ratio):\n",
    "    N = X.shape[0]\n",
    "    sample_size = int(sample_size_ratio*N)\n",
    "    sample_idx = np_rng.choice(np.arange(N), size=sample_size, replace=False)\n",
    "    return X[sample_idx], y[sample_idx]\n",
    "\n",
    "\n",
    "def pca_3d_scatterplot_by_label(data, targets, split_name):\n",
    "    target2lang = {t: l for l, t in lang2target.items()}\n",
    "    \n",
    "    df = pd.DataFrame.from_dict({\n",
    "        \"x\": data[:,0],\n",
    "        \"y\": data[:,1],\n",
    "        \"z\": data[:,2],\n",
    "        \"lang\": [target2lang[t] for t in targets],\n",
    "    })\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    for lang, g in df.groupby(\"lang\"):\n",
    "        ax.scatter(g.x, g.y, g.z, label=lang)\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.set_title(\"3D PCA scatter plot of {} set language vectors\".format(split_name))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "train_X, train_y = embeddings_as_numpy_data(meta[meta[\"split\"]==\"train\"])\n",
    "print(\"training vectors\", train_X.shape, train_y.shape)\n",
    "test_X, test_y = embeddings_as_numpy_data(meta[meta[\"split\"]==\"test\"])\n",
    "print(\"test vectors\", test_X.shape, test_y.shape)\n",
    "\n",
    "# Standardize all vectors using training set statistics\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_X)\n",
    "train_X = scaler.transform(train_X)\n",
    "test_X = scaler.transform(test_X)\n",
    "\n",
    "# Reduce dimensions\n",
    "pre_shape = train_X.shape\n",
    "plda = PLDA()\n",
    "plda.fit(train_X, train_y)\n",
    "train_X = plda.transform(train_X)\n",
    "test_X = plda.transform(test_X)\n",
    "print(\"PLDA reduced dimensions from {} to {}\".format(pre_shape, train_X.shape))\n",
    "\n",
    "# L2-normalize vectors to surface of a unit sphere\n",
    "train_X = normalize(train_X)\n",
    "test_X = normalize(test_X)\n",
    "\n",
    "# Map vectors to 3D with PCA, select 10% samples, plot vectors\n",
    "pca = PCA(n_components=3, whiten=False)\n",
    "pca.fit(train_X)\n",
    "\n",
    "X, y = random_sample(pca.transform(train_X), train_y, 0.1)\n",
    "pca_3d_scatterplot_by_label(X, y, \"training\")\n",
    "\n",
    "X, y = random_sample(pca.transform(test_X), test_y, 0.1)\n",
    "pca_3d_scatterplot_by_label(X, y, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit classifier on training set vectors and evaluate on test set vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from lidbox.util import classification_report\n",
    "\n",
    "\n",
    "# Fit classifier\n",
    "clf = GaussianNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Predict scores on test set with classifier and compute metrics\n",
    "test_pred = clf.predict_log_proba(test_X)\n",
    "# Clamp -infs to -100\n",
    "test_pred = np.maximum(-100, test_pred)\n",
    "report = classification_report(test_y, test_pred, lang2target)\n",
    "display_classification_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Compared to the results from our previous examples, we were unable to get better results by training an RNN based model with the angular proximity loss function.\n",
    "However, the PCA scatter plots suggest that language vectors of the same class are much closer to each other compared to what we extracted from the x-vector model.\n",
    "\n",
    "In any case, we might need much larger datasets before we can reliably compare the x-vector model and the LSTM model we used here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
