{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Automatically reload imported modules that are changed outside this notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# More pixels in figures\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.dpi\"] = 200\n",
    "\n",
    "# Init PRNG with fixed seed for reproducibility\n",
    "import numpy as np\n",
    "np_rng = np.random.default_rng(1)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(np_rng.integers(0, tf.int64.max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Voice spoken language identification with a  neural network\n",
    "\n",
    "**2020-11-08**\n",
    "\n",
    "\n",
    "This example is a thorough, but simple walk-through on how to do everything from loading mp3-files containing speech to preprocessing and transforming the speech data into something we can feed to a neural network classifier.\n",
    "Deep learning based speech analysis is a vast research topic and there are countless techniques that could possibly be applied to improve the results of this example.\n",
    "This example tries to avoid going into too much detail into these techniques and instead focuses on getting an end-to-end classification pipeline up and running with a small dataset.\n",
    "\n",
    "## Data\n",
    "\n",
    "This example uses open speech data downloaded from the [Mozilla Common Voice](https://commonvoice.mozilla.org/en/datasets) project.\n",
    "See the readme file for downloading the data.\n",
    "In addition to the space needed for the downloaded data, you will need at least 10 GiB of free disk space for caching (can be disabled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "languages = \"\"\"\n",
    "    et\n",
    "    mn\n",
    "    ta\n",
    "    tr\n",
    "\"\"\".split()\n",
    "\n",
    "languages = sorted(l.strip() for l in languages)\n",
    "\n",
    "display(Markdown(\"### Languages\"))\n",
    "display(Markdown('\\n'.join(\"* `{}`\".format(l) for l in languages)))\n",
    "\n",
    "bcp47_validator_url = 'https://schneegans.de/lv/?tags='\n",
    "display(Markdown(\"See [this tool]({}) for a description of the BCP-47 language codes.\"\n",
    "                 .format(bcp47_validator_url + urllib.parse.quote('\\n'.join(languages)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the metadata\n",
    "\n",
    "We start by preprocessing the Common Voice metadata files.\n",
    "\n",
    "Update `datadir` and `workdir` to match your setup.\n",
    "All output will be written to `workdir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "workdir = \"/data/exp/cv4\"\n",
    "datadir = \"/mnt/data/speech/common-voice/downloads/2020/cv-corpus\"\n",
    "\n",
    "print(\"work dir:\", workdir)\n",
    "print(\"data source dir:\", datadir)\n",
    "\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "assert os.path.isdir(datadir), datadir + \" does not exist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Voice metadata is distributed as `tsv` files and all audio samples are mp3-files under `clips`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dirs = sorted((f for f in os.scandir(datadir) if f.is_dir()), key=lambda f: f.name)\n",
    "\n",
    "print(datadir)\n",
    "for d in dirs:\n",
    "    if d.name in languages:\n",
    "        print(' ', d.name)\n",
    "        for f in os.scandir(d):\n",
    "            print('   ', f.name)\n",
    "\n",
    "missing_languages = set(languages) - set(d.name for d in dirs)\n",
    "assert missing_languages == set(), \"missing languages: {}\".format(missing_languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's plenty of metadata, but it seems that the train-dev-test split has been predefined so lets use that.\n",
    "\n",
    "[pandas](https://pandas.pydata.org/pandas-docs/stable/index.html) makes it easy to read, filter, and manipulate metadata in tables.\n",
    "Lets try to preprocess all metadata here so we don't have to worry about it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "# Lexicographic order of labels as a fixed index target to label mapping\n",
    "target2lang = tuple(sorted(languages))\n",
    "lang2target = {lang: target for target, lang in enumerate(target2lang)}\n",
    "\n",
    "print(\"lang2target:\", lang2target)\n",
    "print(\"target2lang:\", target2lang)\n",
    "\n",
    "\n",
    "def expand_metadata(row):\n",
    "    \"\"\"\n",
    "    Update dataframe row by generating a unique utterance id,\n",
    "    expanding the absolute path to the mp3 file,\n",
    "    and adding an integer target for the label.\n",
    "    \"\"\"\n",
    "    row.id = \"{:s}_{:s}\".format(\n",
    "        row.path.split(\".mp3\", 1)[0].split(\"common_voice_\", 1)[1],\n",
    "        row.split)\n",
    "    row.path = os.path.join(datadir, row.lang, \"clips\", row.path)\n",
    "    row.target = lang2target[row.lang]\n",
    "    return row\n",
    "\n",
    "\n",
    "def tsv_to_lang_dataframe(lang, split):\n",
    "    \"\"\"\n",
    "    Given a language and dataset split (train, dev, test),\n",
    "    load the Common Voice metadata tsv-file from disk into a pandas.DataFrame.\n",
    "    Preprocess all rows by dropping unneeded columns and adding new metadata.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\n",
    "        os.path.join(datadir, lang, split + \".tsv\"),\n",
    "        sep='\\t',\n",
    "        # We only need these columns from the metadata\n",
    "        usecols=(\"client_id\", \"path\", \"sentence\"))\n",
    "    # Add language label as column\n",
    "    df.insert(len(df.columns), \"lang\", lang)\n",
    "    # Add split name to every row for easier filtering\n",
    "    df.insert(len(df.columns), \"split\", split)\n",
    "    # Add placeholders for integer targets and utterance ids generated row-wise\n",
    "    df.insert(len(df.columns), \"target\", -1)\n",
    "    df.insert(len(df.columns), \"id\", \"\")\n",
    "    # Create new metadata columns\n",
    "    df = df.transform(expand_metadata, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "split_names = (\"train\", \"dev\", \"test\")\n",
    "\n",
    "# Concatenate metadata for all 4 languages into a single table for each split\n",
    "splits = [pd.concat([tsv_to_lang_dataframe(lang, split) for lang in target2lang])\n",
    "          for split in split_names]\n",
    "\n",
    "# Concatenate split metadata into a single table, indexed by utterance ids\n",
    "meta = (pd.concat(splits)\n",
    "        .set_index(\"id\", drop=True, verify_integrity=True)\n",
    "        .sort_index())\n",
    "del splits\n",
    "\n",
    "for split in split_names:\n",
    "    display(Markdown(\"### \" + split))\n",
    "    display(meta[meta[\"split\"]==split])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking that all splits are disjoint by speaker\n",
    "\n",
    "To ensure our neural network will learn what language is being spoken and not who is speaking, we want to test it on data that does not have any voices present in the training data.\n",
    "The `client_id` should correspond to a unique, pseudonymized identifier for every speaker.\n",
    "\n",
    "Lets check all splits are disjoint by speaker id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_splits_disjoint_by_speaker(meta):\n",
    "    split2spk = {split: set(meta[meta[\"split\"]==split].client_id.to_numpy())\n",
    "                 for split in split_names}\n",
    "\n",
    "    for split, spk in split2spk.items():\n",
    "        print(\"split {} has {} speakers\".format(split, len(spk)))\n",
    "\n",
    "    print()\n",
    "    print(\"asserting all are disjoint\")\n",
    "    assert split2spk[\"train\"] & split2spk[\"test\"] == set(), \"train and test, mutual speakers\"\n",
    "    assert split2spk[\"train\"] & split2spk[\"dev\"]  == set(), \"train and dev, mutual speakers\"\n",
    "    assert split2spk[\"dev\"]   & split2spk[\"test\"] == set(), \"dev and test, mutual speakers\"\n",
    "    print(\"ok\")\n",
    "\n",
    "\n",
    "assert_splits_disjoint_by_speaker(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that none of the speakers are in two or more dataset splits.\n",
    "We also see that the test set has a lot of unique speakers who are not in the training set.\n",
    "This is good because we want to test that our neural network classifier knows how to classify input from unknown speakers.\n",
    "\n",
    "### Checking that all audio files exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uttid, row in meta.iterrows():\n",
    "    assert os.path.exists(row[\"path\"]), row[\"path\"] + \" does not exist\"\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing the language distribution\n",
    "\n",
    "Lets see how many samples we have per language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sns.set(rc={'figure.figsize': (8, 6)})\n",
    "ax = sns.countplot(\n",
    "    x=\"split\",\n",
    "    order=split_names,\n",
    "    hue=\"lang\",\n",
    "    hue_order=target2lang,\n",
    "    data=meta)\n",
    "ax.set_title(\"Total amount of audio samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the amount of samples with Mongolian, Tamil, and Turkish speech are quite balanced, but we have significantly larger amounts of Estonian speech.\n",
    "More data is of course always better, but if there is too much of one label compared to the others, our neural network might overfit on this label.\n",
    "\n",
    "But these are only the counts of audio files, how much speech do we have in total per language?\n",
    "We need to read every file to get a reliable answer.\n",
    "See also [SoX](http://sox.sourceforge.net/Main/HomePage) for a good command line tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import miniaudio\n",
    "\n",
    "\n",
    "meta[\"duration\"] = np.array([\n",
    "    miniaudio.mp3_get_file_info(path).duration for path in meta.path], np.float32)\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_duration_distribution(data):\n",
    "    sns.set(rc={'figure.figsize': (8, 6)})\n",
    "    \n",
    "    ax = sns.boxplot(\n",
    "        x=\"split\",\n",
    "        order=split_names,\n",
    "        y=\"duration\",\n",
    "        hue=\"lang\",\n",
    "        hue_order=target2lang,\n",
    "        data=data)\n",
    "    ax.set_title(\"Median audio file duration in seconds\")\n",
    "    plt.show()\n",
    "\n",
    "    ax = sns.barplot(\n",
    "        x=\"split\",\n",
    "        order=split_names,\n",
    "        y=\"duration\",\n",
    "        hue=\"lang\",\n",
    "        hue_order=target2lang,\n",
    "        data=data,\n",
    "        ci=None,\n",
    "        estimator=np.sum)\n",
    "    ax.set_title(\"Total amount of audio in seconds\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_duration_distribution(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median length of Estonian samples is approx. 2.5 seconds greater compared to Turkish samples, which have the shortest median length.\n",
    "We can also see that the total amount of Estonian speech is much larger compared to other languages in our datasets.\n",
    "Notice also the significant amount of outliers with long durations in the Tamil and Turkish datasets.\n",
    "\n",
    "Lets do simple random oversampling for the training split using this approach:\n",
    "\n",
    "1. Select the target language according to maximum total amount of speech in seconds (Estonian).\n",
    "2. Compute differences in total durations between the target language and the three other languages.\n",
    "3. Compute median signal length by language.\n",
    "4. Compute sample sizes by dividing the duration deltas with median signal lengths, separately for each language.\n",
    "5. Draw samples with replacement from the metadata separately for each language.\n",
    "6. Merge samples with rest of the metadata and verify there are no duplicate ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def random_oversampling(meta):\n",
    "    groupby_lang = meta[[\"lang\", \"duration\"]].groupby(\"lang\")\n",
    "    \n",
    "    total_dur = groupby_lang.sum()\n",
    "    target_lang = total_dur.idxmax()[0]\n",
    "    print(\"target lang:\", target_lang)\n",
    "    print(\"total durations:\")\n",
    "    display(total_dur)\n",
    "    \n",
    "    total_dur_delta = total_dur.loc[target_lang] - total_dur\n",
    "    print(\"total duration delta to target lang:\")\n",
    "    display(total_dur_delta)\n",
    "    \n",
    "    median_dur = groupby_lang.median()\n",
    "    print(\"median durations:\")\n",
    "    display(median_dur)\n",
    "    \n",
    "    sample_sizes = (total_dur_delta / median_dur).astype(np.int32)\n",
    "    print(\"median duration weighted sample sizes based on total duration differences:\")\n",
    "    display(sample_sizes)\n",
    "    \n",
    "    samples = []\n",
    "    \n",
    "    for lang in groupby_lang.groups:\n",
    "        sample_size = sample_sizes.loc[lang][0]\n",
    "        sample = (meta[meta[\"lang\"]==lang]\n",
    "                  .sample(n=sample_size, replace=True, random_state=np_rng.bit_generator)\n",
    "                  .reset_index()\n",
    "                  .transform(update_sample_id, axis=1))\n",
    "        samples.append(sample)\n",
    "\n",
    "    return pd.concat(samples).set_index(\"id\", drop=True, verify_integrity=True)\n",
    "\n",
    "\n",
    "def update_sample_id(row):\n",
    "    row[\"id\"] = \"{}_copy_{}\".format(row[\"id\"], row.name)\n",
    "    return row\n",
    "\n",
    "    \n",
    "# Augment training set metadata\n",
    "meta = pd.concat([random_oversampling(meta[meta[\"split\"]==\"train\"]), meta]).sort_index()\n",
    "\n",
    "assert not meta.isna().any(axis=None), \"NaNs in metadata after augmentation\"\n",
    "plot_duration_distribution(meta)\n",
    "assert_splits_disjoint_by_speaker(meta)\n",
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech data augmentation is a common research topic.\n",
    "There are [better](https://www.isca-speech.org/archive/interspeech_2015/papers/i15_3586.pdf) ways to augment data than the simple duplication of metadata rows we did here.\n",
    "One approach (which we won't be doing here) which is easy to implement and might work well is to take copies of signals and make them randomly a bit faster or slower.\n",
    "For example, draw randomly speed ratios from `[0.9, 1.1]` and resample the signal by multiplying its sample rate with the random ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the audio\n",
    "\n",
    "Lets take a look at the speech data and listen to a few randomly picked samples from each label.\n",
    "We pick 2 random samples for each language from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "samples = (meta[meta[\"split\"]==\"train\"]\n",
    "           .groupby(\"lang\")\n",
    "           .sample(n=2, random_state=np_rng.bit_generator))\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then lets read the mp3-files from disk, plot the signals, and listen to the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Audio, HTML\n",
    "import scipy.signal\n",
    "\n",
    "\n",
    "def read_mp3(path, resample_rate=16000):\n",
    "    if isinstance(path, bytes):\n",
    "        # If path is a tf.string tensor, it will be in bytes\n",
    "        path = path.decode(\"utf-8\")\n",
    "        \n",
    "    f = miniaudio.mp3_read_file_f32(path)\n",
    "    \n",
    "    # Downsample to target rate, 16 kHz is commonly used for speech data\n",
    "    new_len = round(len(f.samples) * float(resample_rate) / f.sample_rate)\n",
    "    signal = scipy.signal.resample(f.samples, new_len)\n",
    "    \n",
    "    # Normalize to [-1, 1]\n",
    "    signal /= np.abs(signal).max()\n",
    "    \n",
    "    return signal, resample_rate\n",
    "\n",
    "\n",
    "def embed_audio(signal, rate):\n",
    "    display(Audio(data=signal, rate=rate, embed=True, normalize=False))\n",
    "\n",
    "    \n",
    "def plot_signal(data, figsize=(6, 0.5), **kwargs):\n",
    "    ax = sns.lineplot(data=data, lw=0.1, **kwargs)\n",
    "    ax.set_axis_off()\n",
    "    ax.margins(0)\n",
    "    plt.gcf().set_size_inches(*figsize)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_separator():\n",
    "    display(HTML(data=\"<hr style='border: 2px solid'>\"))\n",
    "\n",
    "    \n",
    "for sentence, lang, clip_path in samples[[\"sentence\", \"lang\", \"path\"]].to_numpy():\n",
    "    signal, rate = read_mp3(clip_path)\n",
    "    plot_signal(signal)\n",
    "    print(\"length: {} sec\".format(signal.size / rate))\n",
    "    print(\"lang:\", lang)\n",
    "    print(\"sentence:\", sentence)\n",
    "    embed_audio(signal, rate)\n",
    "    plot_separator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most challenging aspects of the Mozilla Common Voice dataset is that the audio quality varies greatly: different microphones, background noise, user is speaking close to the device or far away etc.\n",
    "It is difficult to ensure that a neural network will learn to classify different languages as opposed to classifying distinct acoustic artefacts from specific microphones.\n",
    "There's a [vast amount of research](https://www.isca-speech.org/archive/Interspeech_2020/) being done on developing techniques for solving these kind of problems.\n",
    "However, these are well out of scope for this simple example and we won't be studying them here.\n",
    "\n",
    "\n",
    "## Spectral representations\n",
    "\n",
    "It is usually not possible (at least not yet in 2020) to detect languages directly from the waveform.\n",
    "Instead, the [fast Fourier transform](https://en.wikipedia.org/wiki/Short-time_Fourier_transform) (FFT) is applied on small, overlapping windows of the signal to get a 2-dimensional representation of energies in different frequency bands.\n",
    "See [this](https://wiki.aalto.fi/display/ITSP/Spectrogram+and+the+STFT) for further details.\n",
    "\n",
    "However, output from the FFT is usually not usable directly and must be refined.\n",
    "Lets begin by selecting the first signal from our random sample and extract the power spectrogram.\n",
    "\n",
    "### Power spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lidbox.features.audio import spectrograms\n",
    "\n",
    "\n",
    "def plot_spectrogram(S, cmap=\"viridis\", figsize=None, **kwargs):\n",
    "    if figsize is None:\n",
    "        figsize = S.shape[0]/50, S.shape[1]/50\n",
    "    ax = sns.heatmap(S.T, cbar=False, cmap=cmap, **kwargs)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_axis_off()\n",
    "    ax.margins(0)\n",
    "    plt.gcf().set_size_inches(*figsize)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "sample = samples[[\"sentence\", \"lang\", \"path\"]].to_numpy()[0]\n",
    "sentence, lang, clip_path = sample\n",
    "\n",
    "signal, rate = read_mp3(clip_path)\n",
    "plot_signal(signal)\n",
    "\n",
    "powspec = spectrograms([signal], rate)[0]\n",
    "\n",
    "plot_spectrogram(powspec.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This representation is very sparse, with zeros everywhere except in the lowest frequency bands.\n",
    "The main problem here is that relative differences between energy values are very large, making it different to compare large changes in energy.\n",
    "These differences can be reduced by mapping the values onto a logarithmic scale.\n",
    "\n",
    "The [decibel-scale](https://en.wikipedia.org/wiki/Decibel) is a common choice.\n",
    "We will use the maximum value of `powspec` as the reference power ($\\text{P}_0$).\n",
    "\n",
    "### Decibel-scale spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lidbox.features.audio import power_to_db\n",
    "\n",
    "\n",
    "dbspec = power_to_db([powspec])[0]\n",
    "plot_spectrogram(dbspec.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an improvement, but the representation is still rather sparse.\n",
    "We also see that most speech information is in the lower bands, with a bit of energy in the higher frequencies.\n",
    "A common approach is to \"squeeze together\" the y-axis of all frequency bands by using a different scale, such as the [Mel-scale](https://en.wikipedia.org/wiki/Mel_scale).\n",
    "Lets \"squeeze\" the current 256 frequency bins into 40 Mel-bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-scale Mel-spectrogram\n",
    "\n",
    "**Note** that we are scaling different things here.\n",
    "The Mel-scale warps the frequency bins (y-axis), while the logarithm is used to reduce relative differences between individual spectrogram values (pixels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lidbox.features.audio import linear_to_mel\n",
    "\n",
    "\n",
    "def logmelspectrograms(signals, rate):\n",
    "    powspecs = spectrograms(signals, rate)\n",
    "    melspecs = linear_to_mel(powspecs, rate, num_mel_bins=40)\n",
    "    return tf.math.log(melspecs + 1e-6)\n",
    "    \n",
    "\n",
    "logmelspec = logmelspectrograms([signal], rate)[0]\n",
    "plot_spectrogram(logmelspec.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common normalization technique is frequency channel standardization, i.e. normalization of rows to zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lidbox.features import cmvn\n",
    "\n",
    "logmelspec_mv = cmvn([logmelspec])[0]\n",
    "plot_spectrogram(logmelspec_mv.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or only mean-normalization if you think the variances contain important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmelspec_m = cmvn([logmelspec], normalize_variance=False)[0]\n",
    "plot_spectrogram(logmelspec_m.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cepstral representations\n",
    "\n",
    "Another common representation are the Mel-frequency cepstral coefficients (MFCC), which are obtained by applying the [discrete cosine transform](https://en.wikipedia.org/wiki/Discrete_cosine_transform) on the log-scale Mel-spectrogram.\n",
    "\n",
    "### MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_cepstra(X, figsize=None):\n",
    "    if not figsize:\n",
    "        figsize = (X.shape[0]/50, X.shape[1]/20)\n",
    "    plot_spectrogram(X, cmap=\"RdBu_r\", figsize=figsize)\n",
    "\n",
    "    \n",
    "mfcc = tf.signal.mfccs_from_log_mel_spectrograms([logmelspec])[0]\n",
    "plot_cepstra(mfcc.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the information is concentrated in the lower coefficients.\n",
    "It is common to drop the 0th coefficient and select a subset starting at 1, e.g. 1 to 20.\n",
    "See [this post](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = mfcc[:,1:21]\n",
    "plot_cepstra(mfcc.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a very compact representation, but most of the variance is still in the lower coefficients and overshadows the smaller changes in higher coefficients.\n",
    "We can normalize the MFCC matrix row-wise by standardizing each row to zero mean and unit variance.\n",
    "This is commonly called cepstral mean and variance normalization (CMVN).\n",
    "\n",
    "### MFCC + CMVN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_cmvn = cmvn([mfcc])[0]\n",
    "plot_cepstra(mfcc_cmvn.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which one is best?\n",
    "\n",
    "Speech feature extraction is a large, active research topic and it is impossible to choose one representation that would work well in all situations.\n",
    "Common choices in state-of-the-art spoken language identification are log-scale Mel-spectrograms and MFCCs, with different normalization approaches.\n",
    "For example, [here](https://github.com/swshon/dialectID_e2e) is an experiment in Arabic dialect identification, where log-scale Mel-spectra (referred to as FBANK) produced slightly better results compared to MFCCs.\n",
    "\n",
    "It is not obvious when to choose which representation, or if we should even use the FFT at all.\n",
    "You can read [this post](https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html) for a more detailed discussion.\n",
    "\n",
    "## Voice activity detection\n",
    "\n",
    "It is common for speech datasets to contain audio samples with short segments of silence or sounds that are not speech.\n",
    "Since these are usually irrelevant for making a language classification decision, we would prefer to discard such segments.\n",
    "This is called voice activity detection (VAD) and it is another large, active research area.\n",
    "[Here](https://wiki.aalto.fi/pages/viewpage.action?pageId=151500905) is a brief overview of VAD. \n",
    "\n",
    "Non-speech segments can be either noise or silence. \n",
    "Separating non-speech noise from speech is non-trivial but possible, for example with [neural networks](https://www.isca-speech.org/archive/Interspeech_2019/pdfs/1354.pdf).\n",
    "Silence, on the other hand, shows up as zeros in our speech representations, since these segments contain lower energy values compared to segments with speech.\n",
    "Such non-speech segments are therefore easy to detect and discard, for example by comparing the energy of the segment to the average energy of the whole sample.\n",
    "\n",
    "If the samples in our example do not contain much background noise, a simple energy-based VAD technique should be enough to drop all silent segments.\n",
    "We'll use the [root mean square](https://en.wikipedia.org/wiki/Root_mean_square) (RMS) energy to detect short silence segments.\n",
    "`lidbox` has a simple energy-based VAD function, which we will use as follows:\n",
    "\n",
    "1. Divide the signal into non-overlapping 10 ms long windows.\n",
    "2. Compute RMS of each window.\n",
    "3. Reduce all window RMS values by averaging to get a single mean RMS value.\n",
    "4. Set a decision threshold at 0.1 for marking silence windows. In other words, if the window RMS is less than 0.1 of the mean RMS, mark the window as silence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lidbox.features.audio import framewise_rms_energy_vad_decisions\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "sentence, lang, clip_path = sample\n",
    "signal, rate = read_mp3(clip_path)\n",
    "\n",
    "window_ms = tf.constant(10, tf.int32)\n",
    "window_frame_length = (window_ms * rate) // 1000\n",
    "\n",
    "# Get binary VAD decisions for each 10 ms window\n",
    "vad_1 = framewise_rms_energy_vad_decisions(\n",
    "    signal=signal,\n",
    "    sample_rate=rate,\n",
    "    frame_step_ms=window_ms,\n",
    "    strength=0.1)\n",
    "\n",
    "# Plot unfiltered signal\n",
    "sns.set(rc={'figure.figsize': (6, 0.5)})\n",
    "ax = sns.lineplot(data=signal, lw=0.1, legend=None)\n",
    "ax.set_axis_off()\n",
    "ax.margins(0)\n",
    "\n",
    "# Plot shaded area over samples marked as not speech (VAD == 0)\n",
    "for x, is_speech in enumerate(vad_1.numpy()):\n",
    "    if not is_speech:\n",
    "        rect = patches.Rectangle(\n",
    "            (x*window_frame_length, -1),\n",
    "            window_frame_length,\n",
    "            2,\n",
    "            linewidth=0,\n",
    "            color='gray',\n",
    "            alpha=0.2)\n",
    "        ax.add_patch(rect)\n",
    "plt.show()\n",
    "\n",
    "print(\"lang:\", lang)\n",
    "print(\"sentence: '{}'\".format(sentence))\n",
    "embed_audio(signal, rate)\n",
    "\n",
    "# Partition the signal into 10 ms windows to match the VAD decisions\n",
    "windows = tf.signal.frame(signal, window_frame_length, window_frame_length)\n",
    "# Filter signal with VAD decision == 1 (remove gray areas)\n",
    "filtered_signal = tf.reshape(windows[vad_1], [-1])\n",
    "\n",
    "plot_signal(filtered_signal)\n",
    "print(\"dropped {:d} out of {:d} frames, leaving {:.3f} of the original signal\".format(\n",
    "    signal.shape[0] - filtered_signal.shape[0],\n",
    "    signal.shape[0],\n",
    "    filtered_signal.shape[0]/signal.shape[0]))\n",
    "embed_audio(filtered_signal, rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filtered signal has less silence, but some of the pauses between words sound too short and unnatural.\n",
    "We would prefer not to remove small pauses that normally occur between words, so lets say all pauses shorter than 300 ms should not be filtered out.\n",
    "Lets also move all VAD code into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_silence(signal, rate):\n",
    "    window_ms = tf.constant(10, tf.int32)\n",
    "    window_frames = (window_ms * rate) // 1000\n",
    "    \n",
    "    # Get binary VAD decisions for each 10 ms window\n",
    "    vad_1 = framewise_rms_energy_vad_decisions(\n",
    "        signal=signal,\n",
    "        sample_rate=rate,\n",
    "        frame_step_ms=window_ms,\n",
    "        # Do not return VAD = 0 decisions for sequences shorter than 300 ms\n",
    "        min_non_speech_ms=300,\n",
    "        strength=0.1)\n",
    "    \n",
    "    # Partition the signal into 10 ms windows to match the VAD decisions\n",
    "    windows = tf.signal.frame(signal, window_frames, window_frames)\n",
    "    # Filter signal with VAD decision == 1\n",
    "    return tf.reshape(windows[vad_1], [-1])\n",
    "\n",
    "\n",
    "sentence, lang, clip_path = sample\n",
    "signal, rate = read_mp3(clip_path)\n",
    "\n",
    "filtered_signal = remove_silence(signal, rate)\n",
    "plot_signal(filtered_signal)\n",
    "\n",
    "print(\"dropped {:d} out of {:d} frames, leaving {:.3f} of the original signal\".format(\n",
    "    signal.shape[0] - filtered_signal.shape[0],\n",
    "    signal.shape[0],\n",
    "    filtered_signal.shape[0]/signal.shape[0]))\n",
    "\n",
    "print(\"lang:\", lang)\n",
    "print(\"sentence: '{}'\".format(sentence))\n",
    "embed_audio(filtered_signal, rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dropped some silence segments but left most of the speech intact, perhaps this is enough for our example.\n",
    "\n",
    "Although this VAD approach is simple and works ok for our data, it will not work for speech data with non-speech sounds in the background like music or noise.\n",
    "For such data we might need more powerful VAD filters such as neural networks that have been trained on a speech vs non-speech classification task with large amounts of different noise.\n",
    "\n",
    "But lets not add more complexity to our example.\n",
    "We'll use the RMS based filter for all other signals too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of representations\n",
    "\n",
    "Lets extract these features for all signals in our random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for sentence, lang, clip_path in samples[[\"sentence\", \"lang\", \"path\"]].to_numpy():\n",
    "    signal_before_vad, rate = read_mp3(clip_path)\n",
    "    signal = remove_silence(signal_before_vad, rate)\n",
    "    \n",
    "    logmelspec = logmelspectrograms([signal], rate)[0]\n",
    "    logmelspec_mvn = cmvn([logmelspec], normalize_variance=False)[0]\n",
    "    \n",
    "    mfcc = tf.signal.mfccs_from_log_mel_spectrograms([logmelspec])[0]\n",
    "    mfcc = mfcc[:,1:21]\n",
    "    mfcc_cmvn = cmvn([mfcc])[0]\n",
    "    \n",
    "    plot_width = logmelspec.shape[0]/50\n",
    "    plot_signal(signal.numpy(), figsize=(plot_width, .6))\n",
    "    print(\"VAD: {} -> {} sec\".format(\n",
    "        signal_before_vad.size / rate,\n",
    "        signal.numpy().size / rate))\n",
    "    print(\"lang:\", lang)\n",
    "    print(\"sentence:\", sentence)\n",
    "    embed_audio(signal.numpy(), rate)\n",
    "    \n",
    "    plot_spectrogram(logmelspec_mvn.numpy(), figsize=(plot_width, 1.2))\n",
    "    plot_cepstra(mfcc_cmvn.numpy(), figsize=(plot_width, .6))\n",
    "    \n",
    "    plot_separator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the samples to a `tf.data.Dataset` iterator\n",
    "\n",
    "Our dataset is relatively small (2.5 GiB) and we might be able to read all files into signals and keep them in main memory.\n",
    "However, most speech datasets are much larger due to the amount of data needed for training neural network models that would be of any practical use.\n",
    "We need some kind of lazy iteration or streaming solution that views only one part of the dataset at a time.\n",
    "One such solution is to represent the dataset as a [TensorFlow iterator](https://www.tensorflow.org/api_docs/python/tf/data/Dataset), which evaluates its contents only when they are needed, similar to the [MapReduce](https://en.wikipedia.org/wiki/MapReduce) programming model for big data.\n",
    "\n",
    "The downside with lazy iteration or streaming is that we lose the capability of doing random access by row id.\n",
    "However, this shouldn't be a problem since we can always keep the whole metadata table in memory and do random access on its rows whenever needed.\n",
    "\n",
    "Another benefit of TensorFlow dataset iterators is that we can map arbitrary [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)s over the dataset and TensorFlow will automatically parallelize the computations and place them on different devices, such as the GPU.\n",
    "The core architecture of `lidbox` has been organized around the `tf.data.Dataset` API, leaving all the heavy lifting for TensorFlow to handle.\n",
    "\n",
    "But before we load all our speech data, lets warmup with our small random sample of 8 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load it into a `tf.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_to_dataset_input(meta):   \n",
    "    # Create a mapping from column names to all values under the column as tensors\n",
    "    return {\n",
    "        \"id\": tf.constant(meta.index, tf.string),\n",
    "        \"path\": tf.constant(meta.path, tf.string),\n",
    "        \"lang\": tf.constant(meta.lang, tf.string),\n",
    "        \"target\": tf.constant(meta.target, tf.int32),\n",
    "        \"split\": tf.constant(meta.split, tf.string),\n",
    "    }\n",
    "\n",
    "\n",
    "sample_ds = tf.data.Dataset.from_tensor_slices(metadata_to_dataset_input(samples))\n",
    "sample_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All elements produced by the `Dataset` iterator are `dict`s of (string, Tensor) pairs, where the string denotes the metadata type.\n",
    "\n",
    "Although the `Dataset` object is primarily for automating large-scale data processing pipelines, it is easy to extract all elements as `numpy`-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for x in sample_ds.as_numpy_iterator():\n",
    "    display(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading audio files\n",
    "\n",
    "Lets load the signals by [mapping](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) a file reading function for each element over the whole dataset.\n",
    "We'll add a `tf.data.Dataset` function wrapper on top of `read_mp3`, which we defined earlier.\n",
    "TensorFlow will infer the input and output values of the wrapper as tensors from the type signature of dataset elements.\n",
    "We must use `tf.numpy_function` if we want to allow calling the non-TensorFlow function `read_mp3` also from\n",
    "inside the graph environment.\n",
    "It might not be as efficient as using TensorFlow ops but reading a file would have a lot of latency anyway so this is not such a big hit for performance.\n",
    "Besides, we can always hide the latency by reading several files in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def read_mp3_wrapper(x):\n",
    "    signal, sample_rate = tf.numpy_function(\n",
    "        # Function\n",
    "        read_mp3,\n",
    "        # Argument list\n",
    "        [x[\"path\"]],\n",
    "        # Return value types\n",
    "        [tf.float32, tf.int64])\n",
    "    return dict(x, signal=signal, sample_rate=tf.cast(sample_rate, tf.int32))\n",
    "\n",
    "\n",
    "for x in sample_ds.map(read_mp3_wrapper).as_numpy_iterator():\n",
    "    print(\"id: {}\".format(x[\"id\"].decode(\"utf-8\")))\n",
    "    print(\"signal.shape: {}, sample rate: {}\".format(x[\"signal\"].shape, x[\"sample_rate\"]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing silence and extracting features\n",
    "\n",
    "Organizing all preprocessing steps as functions that can be mapped over the `Dataset` object allows us to represent complex transformations easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def remove_silence_wrapper(x):\n",
    "    return dict(x, signal=remove_silence(x[\"signal\"], x[\"sample_rate\"]))\n",
    "\n",
    "\n",
    "def batch_extract_features(x):\n",
    "    with tf.device(\"GPU\"):\n",
    "        signals, rates = x[\"signal\"], x[\"sample_rate\"]\n",
    "        logmelspecs = logmelspectrograms(signals, rates[0])\n",
    "        logmelspecs_smn = cmvn(logmelspecs, normalize_variance=False)\n",
    "        mfccs = tf.signal.mfccs_from_log_mel_spectrograms(logmelspecs)\n",
    "        mfccs = mfccs[...,1:21]\n",
    "        mfccs_cmvn = cmvn(mfccs)\n",
    "    return dict(x, logmelspec=logmelspecs_smn, mfcc=mfccs_cmvn)\n",
    "\n",
    "\n",
    "features_ds = (sample_ds.map(read_mp3_wrapper)\n",
    "                 .map(remove_silence_wrapper)\n",
    "                 .batch(1)\n",
    "                 .map(batch_extract_features)\n",
    "                 .unbatch())\n",
    "\n",
    "for x in features_ds.as_numpy_iterator():\n",
    "    print(x[\"id\"])\n",
    "    for k in (\"signal\", \"logmelspec\", \"mfcc\"):\n",
    "        print(\"{}.shape: {}\".format(k, x[k].shape))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting dataset contents in TensorBoard\n",
    "\n",
    "`lidbox` has a helper function for dumping element information into [`TensorBoard`](https://www.tensorflow.org/tensorboard) summaries.\n",
    "This converts all 2D features into images, writes signals as audio summaries, and extracts utterance ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import lidbox.data.steps as ds_steps\n",
    "\n",
    "\n",
    "cachedir = os.path.join(workdir, \"cache\")\n",
    "\n",
    "_ = ds_steps.consume_to_tensorboard(\n",
    "    # Rename logmelspec as 'input', these will be plotted as images\n",
    "    ds=features_ds.map(lambda x: dict(x, input=x[\"logmelspec\"])),\n",
    "    summary_dir=os.path.join(cachedir, \"tensorboard\", \"data\", \"sample\"),\n",
    "    config={\"batch_size\": 1, \"image_size_multiplier\": 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a terminal and launch TensorBoard to view the summaries written to `$wrkdir/cache/tensorboard/dataset/sample`:\n",
    "```\n",
    "tensorboard --logdir /data/exp/cv4/cache/tensorboard\n",
    "```\n",
    "Then open the url in a browser and inspect the contents.\n",
    "You can leave the server running, since we'll log the training progress to the same directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading all data\n",
    "\n",
    "We'll now begin loading everything from disk and preparing a pipeline from mp3-filepaths to neural network input.\n",
    "We'll use the autotune feature of `tf.data` to allow TensorFlow figure out automatically how much of the pipeline should be split up into parallel calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import lidbox.data.steps as ds_steps\n",
    "\n",
    "TF_AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "def signal_is_not_empty(x):\n",
    "    return tf.size(x[\"signal\"]) > 0\n",
    "    \n",
    "\n",
    "def pipeline_from_metadata(data, shuffle=False):\n",
    "    if shuffle:\n",
    "        # Shuffle metadata to get an even distribution of labels\n",
    "        data = data.sample(frac=1, random_state=np_rng.bit_generator)\n",
    "    ds = (\n",
    "        # Initialize dataset from metadata\n",
    "        tf.data.Dataset.from_tensor_slices(metadata_to_dataset_input(data))\n",
    "        # Read mp3 files from disk in parallel\n",
    "        .map(read_mp3_wrapper, num_parallel_calls=TF_AUTOTUNE)\n",
    "        # Apply RMS VAD to drop silence from all signals\n",
    "        .map(remove_silence_wrapper, num_parallel_calls=TF_AUTOTUNE)\n",
    "        # Drop signals that VAD removed completely\n",
    "        .filter(signal_is_not_empty)\n",
    "        # Extract features in parallel\n",
    "        .batch(1)\n",
    "        .map(batch_extract_features, num_parallel_calls=TF_AUTOTUNE)\n",
    "        .unbatch()\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "\n",
    "# Mapping from dataset split names to tf.data.Dataset objects\n",
    "split2ds = {\n",
    "    split: pipeline_from_metadata(meta[meta[\"split\"]==split], shuffle=split==\"train\")\n",
    "    for split in split_names\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing pipeline performance\n",
    "\n",
    "Note that we only constructed the pipeline with all steps we want to compute.\n",
    "All TensorFlow ops are computed only when elements are requested from the iterator.\n",
    "\n",
    "Lets iterate over the training dataset from first to last element to ensure the pipeline will not be a performance bottleneck during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = ds_steps.consume(split2ds[\"train\"], log_interval=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching pipeline state\n",
    "\n",
    "We can [cache](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache) the iterator state as a single binary file at arbitrary stages.\n",
    "This allows us to automatically skip all steps that precede the call to `tf.Dataset.cache`.\n",
    "\n",
    "Lets cache the training dataset and iterate again over all elements to fill the cache.\n",
    "**Note** that you will still be storing all data on the disk (4.6 GiB new data), so this optimization is a space-time tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(cachedir, \"data\"))\n",
    "\n",
    "split2ds[\"train\"] = split2ds[\"train\"].cache(os.path.join(cachedir, \"data\", \"train\"))\n",
    "_ = ds_steps.consume(split2ds[\"train\"], log_interval=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we iterate over the dataset again, TensorFlow should read all elements from the cache file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ds_steps.consume(split2ds[\"train\"], log_interval=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a side note, if your training environment has fast read-write access to a file system configured for reading and writing very large files, this optimization can be a very significant performance improvement.\n",
    "\n",
    "**Note** also that all usual problems related to cache invalidation apply.\n",
    "When caching extracted features and metadata to disk, be extra careful in your experiments to ensure you are not interpreting results computed on data from some outdated cache.\n",
    "\n",
    "### Dumping a few batches to TensorBoard \n",
    "\n",
    "Lets extract 100 first elements of every split to TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for split, ds in split2ds.items():\n",
    "    _ = ds_steps.consume_to_tensorboard(\n",
    "            ds.map(lambda x: dict(x, input=x[\"logmelspec\"])),\n",
    "            os.path.join(cachedir, \"tensorboard\", \"data\", split),\n",
    "            {\"batch_size\": 1,\n",
    "             \"image_size_multiplier\": 2,\n",
    "             \"num_batches\": 100},\n",
    "            exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a supervised, neural network language classifier\n",
    "\n",
    "We have now configured an efficient data pipeline and extracted some data samples to summary files for TensorBoard.\n",
    "It is time to train a classifier on the data.\n",
    "\n",
    "### Drop metadata from dataset\n",
    "\n",
    "During training, we only need a tuple of model input and targets.\n",
    "We can therefore drop everything else from the dataset elements just before training starts.\n",
    "This is also a good place to decide if we want to train on MFCCs or Mel-spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_input_type = \"logmelspec\"\n",
    "\n",
    "def as_model_input(x):\n",
    "    return x[model_input_type], x[\"target\"]\n",
    "\n",
    "\n",
    "train_ds_demo = list(split2ds[\"train\"]\n",
    "                     .map(as_model_input)\n",
    "                     .shuffle(100)\n",
    "                     .take(6)\n",
    "                     .as_numpy_iterator())\n",
    "\n",
    "for input, target in train_ds_demo:\n",
    "    print(input.shape, target2lang[target])\n",
    "    if model_input_type == \"mfcc\":\n",
    "        plot_cepstra(input)\n",
    "    else:\n",
    "        plot_spectrogram(input)\n",
    "    plot_separator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asserting all input is valid\n",
    "\n",
    "Since the training dataset is cached, we can quickly iterate over all elements and check that we don't have any NaNs or negative targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_finite(x, y):\n",
    "    tf.debugging.assert_all_finite(x, \"non-finite input\")\n",
    "    tf.debugging.assert_non_negative(y, \"negative target\")\n",
    "    return x, y\n",
    "\n",
    "_ = ds_steps.consume(split2ds[\"train\"].map(as_model_input).map(assert_finite), log_interval=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also easy to compute stats on the dataset elements.\n",
    "For example finding global minimum and maximum values of the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_min = split2ds[\"train\"].map(as_model_input).reduce(\n",
    "    tf.float32.max,\n",
    "    lambda acc, elem: tf.math.minimum(acc, tf.math.reduce_min(elem[0])))\n",
    "\n",
    "x_max = split2ds[\"train\"].map(as_model_input).reduce(\n",
    "    tf.float32.min,\n",
    "    lambda acc, elem: tf.math.maximum(acc, tf.math.reduce_max(elem[0])))\n",
    "\n",
    "print(\"input tensor global minimum: {}, maximum: {}\".format(x_min.numpy(), x_max.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a model architecture\n",
    "\n",
    "`lidbox` provides a small set of neural network model architectures out of the box.\n",
    "Many of these architectures have good results in the literature for different datasets.\n",
    "These models have been implemented in Keras, so you could replace the model we are using here with anything you want.\n",
    "\n",
    "The [\"x-vector\"](http://danielpovey.com/files/2018_odyssey_xvector_lid.pdf) architecture has worked well in speaker and language identification so lets create an untrained Keras x-vector model.\n",
    "One of its core features is learning fixed length vector representations (x-vectors) for input of arbitrary length.\n",
    "These vectors are extracted from the first fully connected layer (`segment1`), without activation.\n",
    "This opens up opportunities for doing all kinds of statistical analysis on these vectors, but that's out of scope for our example.\n",
    "\n",
    "We'll try to regularize the network by adding frequency [channel dropout](https://dl.acm.org/doi/abs/10.1016/j.patrec.2017.09.023) with probability 0.8.\n",
    "In other words, during training we set input rows randomly to zeros with probability 0.8.\n",
    "This might avoid overfitting the network on frequency channels containing noise that is irrelevant for deciding the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lidbox.models.xvector as xvector\n",
    "\n",
    "\n",
    "def create_model(num_freq_bins, num_labels):\n",
    "    model = xvector.create([None, num_freq_bins], num_labels, channel_dropout_rate=0.8)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model(\n",
    "    num_freq_bins=20 if model_input_type == \"mfcc\" else 40,\n",
    "    num_labels=len(target2lang))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel dropout demo\n",
    "\n",
    "Here's what happens to the input during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "channel_dropout = tf.keras.layers.SpatialDropout1D(model.get_layer(\"channel_dropout\").rate)\n",
    "\n",
    "for input, target in train_ds_demo:\n",
    "    print(input.shape, target2lang[target])\n",
    "    input = channel_dropout(tf.expand_dims(input, 0), training=True)[0].numpy()\n",
    "    if model_input_type == \"mfcc\":\n",
    "        plot_cepstra(input)\n",
    "    else:\n",
    "        plot_spectrogram(input)\n",
    "    plot_separator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the classifier\n",
    "\n",
    "The validation set is needed after every epoch, so we might as well cache it.\n",
    "**Note** that this writes 2.5 GiB of additional data to disk the first time the validation set is iterated over, i.e. at the end of epoch 1.\n",
    "Also, we can't use batches since our input is of different lengths (perhaps with [ragged tensors](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/data/experimental/dense_to_ragged_batch))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    # Write scalar metrics and network weights to TensorBoard\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(cachedir, \"tensorboard\", model.name),\n",
    "        update_freq=\"epoch\",\n",
    "        write_images=True,\n",
    "        profile_batch=0,\n",
    "    ),\n",
    "    # Stop training if validation loss has not improved from the global minimum in 10 epochs\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "    ),\n",
    "    # Write model weights to cache everytime we get a new global minimum loss value\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(cachedir, \"model\", model.name),\n",
    "        monitor='val_loss',\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "train_ds = split2ds[\"train\"].map(as_model_input).shuffle(1000)\n",
    "dev_ds = split2ds[\"dev\"].cache(os.path.join(cachedir, \"data\", \"dev\")).map(as_model_input)\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds.batch(1),\n",
    "    validation_data=dev_ds.batch(1),\n",
    "    callbacks=callbacks,\n",
    "    verbose=2,\n",
    "    epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the classifier\n",
    "\n",
    "\n",
    "Lets run all test set samples through our trained model by loading the best weights from the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lidbox.util import predict_with_model\n",
    "\n",
    "\n",
    "test_ds = split2ds[\"test\"].map(lambda x: dict(x, input=x[\"logmelspec\"])).batch(1)\n",
    "\n",
    "_ = model.load_weights(os.path.join(cachedir, \"model\", model.name))\n",
    "utt2pred = predict_with_model(model, test_ds)\n",
    "\n",
    "test_meta = meta[meta[\"split\"]==\"test\"]\n",
    "assert not test_meta.join(utt2pred).isna().any(axis=None), \"missing predictions\"\n",
    "test_meta = test_meta.join(utt2pred)\n",
    "test_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average detection cost ($\\text{C}_\\text{avg}$)\n",
    "\n",
    "The de facto standard metric for evaluating spoken language classifiers might be the *average detection cost* ($\\text{C}_\\text{avg}$), which has been refined to its current form during past [language recognition competitions](https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=925272).\n",
    "`lidbox` provides this metric as a `tf.keras.Metric` subclass.\n",
    "Scikit-learn provides other commonly used metrics so there is no need to manually compute those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lidbox.util import classification_report\n",
    "from lidbox.visualize import draw_confusion_matrix\n",
    "\n",
    "\n",
    "true_sparse = test_meta.target.to_numpy(np.int32)\n",
    "\n",
    "pred_dense = np.stack(test_meta.prediction)\n",
    "pred_sparse = pred_dense.argmax(axis=1).astype(np.int32)\n",
    "\n",
    "report = classification_report(true_sparse, pred_dense, lang2target)\n",
    "\n",
    "for m in (\"avg_detection_cost\", \"avg_equal_error_rate\", \"accuracy\"):\n",
    "    print(\"{}: {:.3f}\".format(m, report[m]))\n",
    "    \n",
    "lang_metrics = pd.DataFrame.from_dict({k: v for k, v in report.items() if k in lang2target})\n",
    "lang_metrics[\"mean\"] = lang_metrics.mean(axis=1)\n",
    "display(lang_metrics.T)\n",
    "\n",
    "fig, ax = draw_confusion_matrix(report[\"confusion_matrix\"], lang2target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This was an example on deep learning based simple spoken language identification of 4 different languages from the Mozilla Common Voice free speech datasets.\n",
    "We managed to train a model that adequately recognizes languages spoken by the test set speakers.\n",
    "\n",
    "However, there is clearly room for improvement.\n",
    "We did simple random oversampling to balance the language distribution in the training set, but perhaps there are better ways to do this.\n",
    "We also did not tune optimization hyperparameters or try different neural network architectures or layer combinations.\n",
    "It might also be possible to increase robustness by audio feature engineering, such as [random FIR filtering](https://www.isca-speech.org/archive/Interspeech_2018/abstracts/1047.html) to simulate microphone differences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
